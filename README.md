# 104-Job-Scraper
Pythonè‡ªå‹•åŒ–çˆ¬èŸ²å·¥å…·ï¼Œç”¨æ–¼æœå°‹æŒ‡å®šé—œéµå­—èˆ‡åœ°å€çš„è·ç¼ºï¼Œä¸¦è¼¸å‡ºæˆ CSV æª”æ¡ˆèˆ‡ç¤¾è¦ºåŒ–å‘ˆç¾è·ç¼ºå…±é€šå±¬æ€§ã€‚æ”¯æ´äº’å‹•å¼åœ°å€é¸æ“‡ã€éŒ¯èª¤è™•ç†èˆ‡è·ç¼ºè©³ç´°è§£æã€‚

## ä½¿ç”¨æŠ€è¡“
- Python 3
- requests
- BeautifulSoup4
- pandas
- re / os / time æ¨¡çµ„

## åŠŸèƒ½ç‰¹è‰²
- è‡ªå‹•æ“·å– 104 ç¶²ç«™è·ç¼ºè³‡æ–™ ï¼ˆå¯æŒ‡å®šé—œéµå­—èˆ‡åœ°å€ï¼‰
![image](https://github.com/user-attachments/assets/25890075-0618-4ba4-8f66-a4e386f24204)

- è§£æè©³ç´°è·ç¼ºæ¢ä»¶ï¼ˆå­¸æ­·ã€è–ªè³‡ã€æŠ€èƒ½ã€èªè¨€ç­‰ï¼‰
- è¼¸å‡ºç‚ºå¯åˆ†æçš„ CSV è¡¨æ ¼
![image](https://github.com/user-attachments/assets/d9d9a0b8-185b-43c0-bc12-e0710969255f)

- è‡ªå‹•é–‹å•Ÿçµæœï¼Œå‹å–„ä½¿ç”¨è€…äº’å‹•ä»‹é¢
- è¦–è¦ºåŒ–å‘ˆç¾çˆ¬å–è·ç¼ºçš„å…±é€šç‰¹æ€§ ï¼ˆæ“…é•·å·¥å…·ã€å·¥ä½œæŠ€èƒ½ç­‰ï¼‰

## åŸ·è¡Œæ–¹å¼

```bash
python 104_job_scraper.py

import requests
import pandas as pd
import time
import re
import os
import webbrowser
import platform
from bs4 import BeautifulSoup

headers = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://www.104.com.tw/jobs/search/"
}

area_map = {
    "å°åŒ—å¸‚": "6001001000",
    "æ–°åŒ—å¸‚": "6001002000",
    "æ¡ƒåœ’å¸‚": "6001005000",
    "å°ä¸­å¸‚": "6001008000",
    "å°å—å¸‚": "6001010000",
    "é«˜é›„å¸‚": "6001012000",
    "ä¸é™åœ°å€": "6001000000"
}

def fetch_jobs(keyword, pages=4, area="6001008000"):
    all_jobs = []
    for page in range(pages):
        print(f"ğŸ” æ“·å–ç¬¬ {page+1} é ...")
        url = "https://www.104.com.tw/jobs/search/list"
        params = {
            "ro": "0",
            "kwop": "7",
            "keyword": keyword,
            "order": "11",
            "asc": "0",
            "page": page + 1,
            "mode": "s",
            "jobsource": "2018indexpoc",
            "area": area
        }
        res = requests.get(url, headers=headers, params=params)
        if res.status_code == 200:
            data = res.json()
            jobs = data.get("data", {}).get("list", [])
            all_jobs.extend(jobs)
        else:
            print(f"âŒ ç¬¬ {page+1} é æ“·å–å¤±æ•—")
        time.sleep(1)
    return all_jobs

def extract_job_id(url):
    match = re.search(r'/job/(\w+)', url)
    return match.group(1) if match else None

def fetch_job_detail(job_id):
    url = f"https://www.104.com.tw/job/ajax/content/{job_id}"
    res = requests.get(url, headers=headers)
    if res.status_code == 200:
        data = res.json()
        return data.get("data", {})
    else:
        return None

def parse_job(detail):
    specialties = detail.get("condition", {}).get("specialty", [])
    specialty_str = ', '.join(s.get("description", "") for s in specialties) if specialties else "ç„¡"

    skills = detail.get("condition", {}).get("skill", [])
    skill_str = ', '.join(s.get("description", "") for s in skills) if skills else "ç„¡"

    # é¿å… BeautifulSoup ç©ºå€¼éŒ¯èª¤
    job_description_html = detail.get("jobDetail", {}).get("jobDescription", "")
    job_description = BeautifulSoup(job_description_html, "html.parser").get_text(strip=True) if job_description_html else "ç„¡"

    return {
        "è·ç¼ºåç¨±": detail.get("header", {}).get("jobName", "ç„¡"),
        "å…¬å¸åç¨±": detail.get("header", {}).get("custName", "ç„¡"),
        "å·¥ä½œå¾…é‡": detail.get("jobDetail", {}).get("salary", "ç„¡"),
        "å­¸æ­·è¦æ±‚": detail.get("condition", {}).get("edu", "ç„¡"),
        "å·¥ä½œç¶“æ­·": detail.get("condition", {}).get("workExp", "ç„¡"),
        "ç§‘ç³»è¦æ±‚": ', '.join(detail.get("condition", {}).get("major", [])) or "ç„¡",
        "èªæ–‡æ¢ä»¶": ', '.join(
            f"{lang['language']}({lang['ability']})"
            for lang in detail.get("condition", {}).get("language", [])
        ) if detail.get("condition", {}).get("language") else "ç„¡",
        "æ“…é•·å·¥å…·": specialty_str,
        "å·¥ä½œæŠ€èƒ½": skill_str,
        "å…¶ä»–æ¢ä»¶": detail.get("condition", {}).get("other", "ç„¡"),
        "å·¥ä½œå…§å®¹": job_description
    }

def open_file(filepath):
    try:
        if platform.system() == "Darwin":  # macOS
            os.system(f"open \"{filepath}\"")
        elif platform.system() == "Windows":
            os.startfile(filepath)
        elif platform.system() == "Linux":
            os.system(f"xdg-open \"{filepath}\"")
    except Exception as e:
        print(f"âŒ ç„¡æ³•è‡ªå‹•é–‹å•Ÿæª”æ¡ˆï¼š{e}")

def main():
    start_time = time.time()

    # ä½¿ç”¨è€…è¼¸å…¥
    keyword = input("è«‹è¼¸å…¥è·ç¼ºé—œéµå­—ï¼ˆä¾‹å¦‚ï¼šè³‡æ–™åˆ†æã€Pythonï¼‰ï¼š").strip()
    print("\nğŸ“ è«‹é¸æ“‡åœ°å€ï¼š")
    for idx, city in enumerate(area_map.keys(), 1):
        print(f"{idx}. {city}")
    while True:
        try:
            area_idx = int(input("â¡ï¸ è¼¸å…¥åœ°å€ç·¨è™Ÿï¼š"))
            area = list(area_map.values())[area_idx - 1]
            break
        except (ValueError, IndexError):
            print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„åœ°å€ç·¨è™Ÿï¼")

    # æ“·å–è·ç¼ºåˆ—è¡¨
    jobs = fetch_jobs(keyword, pages=4, area=area)
    print(f"\nğŸ“¦ å…±æ“·å–åˆ° {len(jobs)} ç­†è·ç¼º")

    result = []
    for job in jobs:
        job_url = job.get("link", {}).get("job", "")
        job_id = extract_job_id(job_url)
        if not job_id:
            print(f"âš ï¸ è·³éï¼šç„¡æ³•è§£æ ID â†’ {job}")
            continue
        print(f"â†’ è™•ç†è·ç¼ºï¼š{job.get('jobName')}ï¼ˆID: {job_id}ï¼‰")
        detail = fetch_job_detail(job_id)
        if detail:
            job_info = parse_job(detail)
            result.append(job_info)
        else:
            print(f"âŒ æŠ“å–è·ç¼º {job_id} è©³ç´°å…§å®¹å¤±æ•—")
        time.sleep(0.5)

    # å„²å­˜çµæœ
    df = pd.DataFrame(result)
    filename = f"104_job_list_{keyword}.csv"
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"\nâœ… å„²å­˜æˆåŠŸï¼š{filename}")

    # è‡ªå‹•é–‹å•Ÿ
    full_path = os.path.abspath(filename)
    open_file(full_path)

    # çµ±è¨ˆæ™‚é–“
    duration = round(time.time() - start_time, 2)
    print(f"\nğŸ‰ å®Œæˆï¼å…±æˆåŠŸæ“·å– {len(result)} ç­†è·ç¼ºï¼Œè€—æ™‚ {duration} ç§’ã€‚")

if __name__ == "__main__":
    main()

```
